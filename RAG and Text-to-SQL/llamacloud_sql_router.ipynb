{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining RAG and Text-to-SQL in a Single Query Interface\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llamacloud-demo/blob/main/examples/advanced_rag/llamacloud_sql_router.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This notebook shows you how to create a custom agent that can query either your LlamaCloud index for RAG-based retrieval or a separate SQL query engine as a tool. In this example, we'll use PDFs of Wikipedia pages of US cities and a SQL database of their populations and states as documents.\n",
    "\n",
    "![](llamacloud_sql_router_img.png)\n",
    "\n",
    "**NOTE**: Any Text-to-SQL application should be aware that executing arbitrary SQL queries can be a security risk. It is recommended to take precautions as needed, such as using restricted roles, read-only databases, sandboxing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Set up your OpenAI API key and `nest_asyncio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Setup Observability\n",
    "\n",
    "We setup an integration with LlamaTrace (integration with Arize).\n",
    "\n",
    "If you haven't already done so, make sure to create an account here: https://llamatrace.com/login. Then create an API key and put it in the `PHOENIX_API_KEY` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-callbacks-arize-phoenix\n",
      "  Using cached llama_index_callbacks_arize_phoenix-0.1.3-py3-none-any.whl.metadata (784 bytes)\n",
      "Collecting arize-phoenix>=3.0.3 (from llama-index-callbacks-arize-phoenix)\n",
      "  Using cached arize_phoenix-8.12.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.1 (from llama-index-callbacks-arize-phoenix)\n",
      "  Using cached llama_index_core-0.10.68.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting openinference-instrumentation-llama-index>=1.0.0 (from llama-index-callbacks-arize-phoenix)\n",
      "  Using cached openinference_instrumentation_llama_index-3.3.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting aioitertools (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting aiosqlite (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting alembic<2,>=1.3.0 (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting arize-phoenix-client (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached arize_phoenix_client-1.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting arize-phoenix-evals>=0.13.1 (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached arize_phoenix_evals-0.20.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting arize-phoenix-otel>=0.5.1 (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached arize_phoenix_otel-0.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting authlib (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached authlib-1.5.1-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: cachetools in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (5.5.2)\n",
      "Collecting fastapi (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting grpc-interceptor (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting grpcio (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Downloading grpcio-1.71.0-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: httpx in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (0.28.1)\n",
      "Requirement already satisfied: jinja2 in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (3.1.6)\n",
      "Requirement already satisfied: numpy!=2.0.0 in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (2.2.3)\n",
      "Collecting openinference-instrumentation>=0.1.12 (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached openinference_instrumentation-0.1.23-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting openinference-semantic-conventions>=0.1.12 (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached openinference_semantic_conventions-0.1.14-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting opentelemetry-exporter-otlp (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached opentelemetry_exporter_otlp-1.30.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-proto>=1.12.0 (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0 in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (2.2.3)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.20.2 in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (5.29.3)\n",
      "Requirement already satisfied: psutil in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (19.0.1)\n",
      "Requirement already satisfied: pydantic>=2.1.0 in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (2.10.6)\n",
      "Collecting python-multipart (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting scikit-learn (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached scipy-1.15.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in .\\venv\\lib\\site-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (2.0.38)\n",
      "Collecting sqlean-py>=3.45.1 (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached sqlean.py-3.47.0-cp313-cp313-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting starlette (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting strawberry-graphql>=0.262.0 (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached strawberry_graphql-0.262.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (4.12.2)\n",
      "Collecting uvicorn (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: wrapt>=1.17.2 in .\\venv\\lib\\site-packages (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix) (1.17.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in .\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-callbacks-arize-phoenix) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in .\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-callbacks-arize-phoenix) (3.11.13)\n",
      "Requirement already satisfied: dataclasses-json in .\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-callbacks-arize-phoenix) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in .\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-callbacks-arize-phoenix) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in .\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-callbacks-arize-phoenix) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-callbacks-arize-phoenix) (2025.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in .\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-callbacks-arize-phoenix) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in .\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-callbacks-arize-phoenix) (3.4.2)\n",
      "Requirement already satisfied: nltk!=3.9,>=3.8.1 in .\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-callbacks-arize-phoenix) (3.9.1)\n",
      "Collecting numpy!=2.0.0 (from arize-phoenix>=3.0.3->llama-index-callbacks-arize-phoenix)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [19 lines of output]\n",
      "      + F:\\dailydoseofds_texttosql\\venv\\Scripts\\python.exe C:\\Users\\Mohammad Ullah Riyad\\AppData\\Local\\Temp\\pip-install-1zjv3egn\\numpy_0e02ec0014d74c40a1d69e20673c1e7f\\vendored-meson\\meson\\meson.py setup C:\\Users\\Mohammad Ullah Riyad\\AppData\\Local\\Temp\\pip-install-1zjv3egn\\numpy_0e02ec0014d74c40a1d69e20673c1e7f C:\\Users\\Mohammad Ullah Riyad\\AppData\\Local\\Temp\\pip-install-1zjv3egn\\numpy_0e02ec0014d74c40a1d69e20673c1e7f\\.mesonpy-2kq1kjsz -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\Mohammad Ullah Riyad\\AppData\\Local\\Temp\\pip-install-1zjv3egn\\numpy_0e02ec0014d74c40a1d69e20673c1e7f\\.mesonpy-2kq1kjsz\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.99\n",
      "      Source dir: C:\\Users\\Mohammad Ullah Riyad\\AppData\\Local\\Temp\\pip-install-1zjv3egn\\numpy_0e02ec0014d74c40a1d69e20673c1e7f\n",
      "      Build dir: C:\\Users\\Mohammad Ullah Riyad\\AppData\\Local\\Temp\\pip-install-1zjv3egn\\numpy_0e02ec0014d74c40a1d69e20673c1e7f\\.mesonpy-2kq1kjsz\n",
      "      Build type: native build\n",
      "      Project name: NumPy\n",
      "      Project version: 1.26.4\n",
      "      C compiler for the host machine: gcc (gcc 6.3.0 \"gcc (MinGW.org GCC-6.3.0-1) 6.3.0\")\n",
      "      C linker for the host machine: gcc ld.bfd 2.28\n",
      "      C++ compiler for the host machine: c++ (gcc 6.3.0 \"c++ (MinGW.org GCC-6.3.0-1) 6.3.0\")\n",
      "      C++ linker for the host machine: c++ ld.bfd 2.28\n",
      "      Cython compiler for the host machine: cython (cython 3.0.12)\n",
      "      Host machine cpu family: x86\n",
      "      Host machine cpu: x86\n",
      "      \n",
      "      ..\\meson.build:28:4: ERROR: Problem encountered: NumPy requires GCC >= 8.4\n",
      "      \n",
      "      A full log can be found at C:\\Users\\Mohammad Ullah Riyad\\AppData\\Local\\Temp\\pip-install-1zjv3egn\\numpy_0e02ec0014d74c40a1d69e20673c1e7f\\.mesonpy-2kq1kjsz\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-index-callbacks-arize-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ArizePhoenixCallbackHandler is not installed. Please install it using `pip install llama-index-callbacks-arize-phoenix`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\dailydoseofds_texttosql\\venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\global_handlers.py:46\u001b[39m, in \u001b[36mcreate_global_handler\u001b[39m\u001b[34m(eval_mode, **eval_params)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marize_phoenix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     47\u001b[39m         arize_phoenix_callback_handler,\n\u001b[32m     48\u001b[39m     )  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_index.callbacks'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m PHOENIX_API_KEY = \u001b[33m\"\u001b[39m\u001b[33m<phoenix_api_key>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mOTEL_EXPORTER_OTLP_HEADERS\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mapi_key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPHOENIX_API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mllama_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_global_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marize_phoenix\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://llamatrace.com/v1/traces\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\dailydoseofds_texttosql\\venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\global_handlers.py:10\u001b[39m, in \u001b[36mset_global_handler\u001b[39m\u001b[34m(eval_mode, **eval_params)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Set global eval handlers.\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m handler = \u001b[43mcreate_global_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meval_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handler:\n\u001b[32m     12\u001b[39m     llama_index.core.global_handler = handler\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\dailydoseofds_texttosql\\venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\global_handlers.py:50\u001b[39m, in \u001b[36mcreate_global_handler\u001b[39m\u001b[34m(eval_mode, **eval_params)\u001b[39m\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marize_phoenix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     47\u001b[39m             arize_phoenix_callback_handler,\n\u001b[32m     48\u001b[39m         )  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     51\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mArizePhoenixCallbackHandler is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease install it using `pip install llama-index-callbacks-arize-phoenix`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m         )\n\u001b[32m     55\u001b[39m     handler = arize_phoenix_callback_handler(**eval_params)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m eval_mode == \u001b[33m\"\u001b[39m\u001b[33mhoneyhive\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mImportError\u001b[39m: ArizePhoenixCallbackHandler is not installed. Please install it using `pip install llama-index-callbacks-arize-phoenix`"
     ]
    }
   ],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import llama_index.core\n",
    "import os\n",
    "\n",
    "PHOENIX_API_KEY = \"<phoenix_api_key>\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents into LlamaCloud\n",
    "\n",
    "Download the following Wikipedia pages into PDFs by either pressing Ctrl-P/Cmd-P or right-clicking and selecting \"Print\" and then \"Save as PDF\" as the destination.\n",
    "- [New York City](https://en.wikipedia.org/wiki/New_York_City)\n",
    "- [Los Angeles](https://en.wikipedia.org/wiki/Los_Angeles)\n",
    "- [Chicago](https://en.wikipedia.org/wiki/Chicago)\n",
    "- [Houston](https://en.wikipedia.org/wiki/Houston)\n",
    "- [Miami](https://en.wikipedia.org/wiki/Miami)\n",
    "- [Seattle](https://en.wikipedia.org/wiki/Seattle)\n",
    "\n",
    "After that, create a new index in LlamaCloud and upload your PDFs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Database\n",
    "\n",
    "The SQL database in this example will be created in memory and will contain three columns: the city name, the city's population, and the state the city is located in. The table creation and the information for each city is shown in the snippets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SQLDatabase, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    ")\n",
    "\n",
    "Settings.llm = OpenAI(\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\", future=True)\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "# create city SQL table\n",
    "table_name = \"city_stats\"\n",
    "city_stats_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"state\", String(16), nullable=False),\n",
    ")\n",
    "\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('New York City', 8336000, 'New York'), ('Los Angeles', 3822000, 'California'), ('Chicago', 2665000, 'Illinois'), ('Houston', 2303000, 'Texas'), ('Miami', 449514, 'Florida'), ('Seattle', 749256, 'Washington')]\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import insert\n",
    "from sqlalchemy.dialects.sqlite import insert as sqlite_insert\n",
    "\n",
    "# Sample data\n",
    "rows = [\n",
    "    {\"city_name\": \"New York City\", \"population\": 8336000, \"state\": \"New York\"},\n",
    "    {\"city_name\": \"Los Angeles\", \"population\": 3822000, \"state\": \"California\"},\n",
    "    {\"city_name\": \"Chicago\", \"population\": 2665000, \"state\": \"Illinois\"},\n",
    "    {\"city_name\": \"Houston\", \"population\": 2303000, \"state\": \"Texas\"},\n",
    "    {\"city_name\": \"Miami\", \"population\": 449514, \"state\": \"Florida\"},\n",
    "    {\"city_name\": \"Seattle\", \"population\": 749256, \"state\": \"Washington\"},\n",
    "]\n",
    "\n",
    "# Insert rows with conflict handling\n",
    "for row in rows:\n",
    "    stmt = sqlite_insert(city_stats_table).values(**row)\n",
    "    stmt = stmt.on_conflict_do_update(\n",
    "        index_elements=['city_name'],  # Column(s) with the UNIQUE constraint\n",
    "        set_=row  # Update the row with the new values if a conflict occurs\n",
    "    )\n",
    "    with engine.begin() as connection:\n",
    "        connection.execute(stmt)\n",
    "\n",
    "# Fetch and print all rows from the table\n",
    "with engine.connect() as connection:\n",
    "    cursor = connection.exec_driver_sql(\"SELECT * FROM city_stats\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a query engine based on SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "sql_query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    tables=[\"city_stats\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaCloud Index\n",
    "\n",
    "Create an index and a query engine around the index you've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "    name=\"<Your Index Name>\", \n",
    "    project_name=\"<Your Project Name>\",\n",
    "    organization_id=\"<Your Org ID>\",\n",
    "    api_key=\"<Your API Key>\"\n",
    ")\n",
    "\n",
    "llama_cloud_query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a query engine tool around these query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "sql_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=sql_query_engine,\n",
    "    description=(\n",
    "        \"Useful for translating a natural language query into a SQL query over\"\n",
    "        \" a table containing: city_stats, containing the population/state of\"\n",
    "        \" each city located in the USA.\"\n",
    "    ),\n",
    "    name=\"sql_tool\"\n",
    ")\n",
    "\n",
    "cities = [\"New York City\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Miami\", \"Seattle\"]\n",
    "llama_cloud_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=llama_cloud_query_engine,\n",
    "    description=(\n",
    "        f\"Useful for answering semantic questions about certain cities in the US.\"\n",
    "    ),\n",
    "    name=\"llama_cloud_tool\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Agent Around the Query Engines\n",
    "\n",
    "We'll create a workflow that acts as an agent around the two query engines. In this workflow, we need four events:\n",
    "1. `GatherToolsEvent`: Gets all tools that need to be called (which is determined by the LLM).\n",
    "2. `ToolCallEvent`: An individual tool call. Multiple of these events will be triggered at the same time.\n",
    "3. `ToolCallEventResult`: Gets result from a tool call.\n",
    "4. `GatherEvent`: Returned from dispatcher that triggers the `ToolCallEvent`.\n",
    "\n",
    "This workflow consists of the following steps:\n",
    "1. `chat()`: Appends the message to the chat history. This chat history is fed into the LLM, along with the given tools, and the LLM determines which tools to call. This returns a `GatherToolsEvent`.\n",
    "2. `dispatch_calls()`: Triggers a `ToolCallEvent` for each tool call given in the `GatherToolsEvent` using `send_event()`. Returns a `GatherEvent` with the number of tool calls.\n",
    "3. `call_tool()`: Calls an individual tool. This step will run multiple times if there is more than one tool call. This step calls the tool and appends the result as a chat message to the chat history. It returns a `ToolCallEventResult` with the result of the tool call.\n",
    "4. `gather()`: Gathers the results from all tool calls using `collect_events()`. Waits for all tool calls to finish, then feeds chat history (following all tool calls) into the LLM. Returns the response from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "from llama_index.core.tools import BaseTool\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.llms.llm import ToolSelection, LLM\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "class InputEvent(Event):\n",
    "    \"\"\"Input event.\"\"\"\n",
    "\n",
    "class GatherToolsEvent(Event):\n",
    "    \"\"\"Gather Tools Event\"\"\"\n",
    "\n",
    "    tool_calls: Any\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    \"\"\"Tool Call event\"\"\"\n",
    "\n",
    "    tool_call: ToolSelection\n",
    "\n",
    "class ToolCallEventResult(Event):\n",
    "    \"\"\"Tool call event result.\"\"\"\n",
    "\n",
    "    msg: ChatMessage\n",
    "\n",
    "class RouterOutputAgentWorkflow(Workflow):\n",
    "    \"\"\"Custom router output agent workflow.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        tools: List[BaseTool],\n",
    "        timeout: Optional[float] = 10.0,\n",
    "        disable_validation: bool = False,\n",
    "        verbose: bool = False,\n",
    "        llm: Optional[LLM] = None,\n",
    "        chat_history: Optional[List[ChatMessage]] = None,\n",
    "    ):\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "\n",
    "        super().__init__(timeout=timeout, disable_validation=disable_validation, verbose=verbose)\n",
    "\n",
    "        self.tools: List[BaseTool] = tools\n",
    "        self.tools_dict: Optional[Dict[str, BaseTool]] = {tool.metadata.name: tool for tool in self.tools}\n",
    "        self.llm: LLM = llm or OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "        self.chat_history: List[ChatMessage] = chat_history or []\n",
    "    \n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets Chat History\"\"\"\n",
    "\n",
    "        self.chat_history = []\n",
    "\n",
    "    @step()\n",
    "    async def prepare_chat(self, ev: StartEvent) -> InputEvent:\n",
    "        message = ev.get(\"message\")\n",
    "        if message is None:\n",
    "            raise ValueError(\"'message' field is required.\")\n",
    "        \n",
    "        # add msg to chat history\n",
    "        chat_history = self.chat_history\n",
    "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
    "        return InputEvent()\n",
    "\n",
    "    @step()\n",
    "    async def chat(self, ev: InputEvent) -> GatherToolsEvent | StopEvent:\n",
    "        \"\"\"Appends msg to chat history, then gets tool calls.\"\"\"\n",
    "\n",
    "        # Put msg into LLM with tools included\n",
    "        chat_res = await self.llm.achat_with_tools(\n",
    "            self.tools,\n",
    "            chat_history=self.chat_history,\n",
    "            verbose=self._verbose,\n",
    "            allow_parallel_tool_calls=True\n",
    "        )\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(chat_res, error_on_no_tool_call=False)\n",
    "        \n",
    "        ai_message = chat_res.message\n",
    "        self.chat_history.append(ai_message)\n",
    "        if self._verbose:\n",
    "            print(f\"Chat message: {ai_message.content}\")\n",
    "\n",
    "        # no tool calls, return chat message.\n",
    "        if not tool_calls:\n",
    "            return StopEvent(result=ai_message.content)\n",
    "\n",
    "        return GatherToolsEvent(tool_calls=tool_calls)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def dispatch_calls(self, ctx: Context, ev: GatherToolsEvent) -> ToolCallEvent:\n",
    "        \"\"\"Dispatches calls.\"\"\"\n",
    "\n",
    "        tool_calls = ev.tool_calls\n",
    "        await ctx.set(\"num_tool_calls\", len(tool_calls))\n",
    "\n",
    "        # trigger tool call events\n",
    "        for tool_call in tool_calls:\n",
    "            ctx.send_event(ToolCallEvent(tool_call=tool_call))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @step()\n",
    "    async def call_tool(self, ev: ToolCallEvent) -> ToolCallEventResult:\n",
    "        \"\"\"Calls tool.\"\"\"\n",
    "\n",
    "        tool_call = ev.tool_call\n",
    "\n",
    "        # get tool ID and function call\n",
    "        id_ = tool_call.tool_id\n",
    "\n",
    "        if self._verbose:\n",
    "            print(f\"Calling function {tool_call.tool_name} with msg {tool_call.tool_kwargs}\")\n",
    "\n",
    "        # call function and put result into a chat message\n",
    "        tool = self.tools_dict[tool_call.tool_name]\n",
    "        output = await tool.acall(**tool_call.tool_kwargs)\n",
    "        msg = ChatMessage(\n",
    "            name=tool_call.tool_name,\n",
    "            content=str(output),\n",
    "            role=\"tool\",\n",
    "            additional_kwargs={\n",
    "                \"tool_call_id\": id_,\n",
    "                \"name\": tool_call.tool_name\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return ToolCallEventResult(msg=msg)\n",
    "    \n",
    "    @step(pass_context=True)\n",
    "    async def gather(self, ctx: Context, ev: ToolCallEventResult) -> StopEvent | None:\n",
    "        \"\"\"Gathers tool calls.\"\"\"\n",
    "        # wait for all tool call events to finish.\n",
    "        tool_events = ctx.collect_events(ev, [ToolCallEventResult] * await ctx.get(\"num_tool_calls\"))\n",
    "        if not tool_events:\n",
    "            return None\n",
    "        \n",
    "        for tool_event in tool_events:\n",
    "            # append tool call chat messages to history\n",
    "            self.chat_history.append(tool_event.msg)\n",
    "        \n",
    "        # # after all tool calls finish, pass input event back, restart agent loop\n",
    "        return InputEvent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the workflow instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = RouterOutputAgentWorkflow(tools=[sql_tool, llama_cloud_tool], verbose=True, timeout=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammad Ullah Riyad\\AppData\\Local\\Temp\\ipykernel_5744\\980914799.py:4: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(RouterOutputAgentWorkflow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflow_all_flows.html\n"
     ]
    }
   ],
   "source": [
    "#from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.core.workflow import draw_all_possible_flows  # Updated import\n",
    "\n",
    "draw_all_possible_flows(RouterOutputAgentWorkflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat\n",
      "Step prepare_chat produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced no event\n",
      "Running step call_tool\n",
      "Calling function sql_tool with msg {'input': 'SELECT city FROM city_stats ORDER BY population DESC LIMIT 1'}\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: New York City has the highest population.\n",
      "Step chat produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "New York City has the highest population."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "result = await wf.run(message=\"Which city has the highest population?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat\n",
      "Step prepare_chat produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced no event\n",
      "Running step call_tool\n",
      "Calling function llama_cloud_tool with msg {'input': 'What state is Houston located in?'}\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: Houston is located in the state of Texas.\n",
      "Step chat produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Houston is located in the state of Texas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await wf.run(message=\"What state is Houston located in?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat\n",
      "Step prepare_chat produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced no event\n",
      "Running step call_tool\n",
      "Calling function llama_cloud_tool with msg {'input': 'Where is the Space Needle located?'}\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: The Space Needle is located in Seattle, Washington.\n",
      "Step chat produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Space Needle is located in Seattle, Washington."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await wf.run(message=\"Where is the Space Needle located?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat\n",
      "Step prepare_chat produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced no event\n",
      "Running step call_tool\n",
      "Calling function llama_cloud_tool with msg {'input': 'List all of the places to visit in Miami'}\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: Here are some places to visit in Miami:\n",
      "- Beaches and parks\n",
      "- Zoo Miami\n",
      "- Jungle Island\n",
      "- Miami Seaquarium\n",
      "- Botanic Garden\n",
      "- Key Biscayne\n",
      "- South Beach\n",
      "- Lincoln Road\n",
      "- Bayside Marketplace\n",
      "- Downtown Miami\n",
      "- Brickell City Centre\n",
      "- Art Deco District in Miami Beach\n",
      "- Miami Open\n",
      "- Art Basel\n",
      "- Winter Music Conference\n",
      "- South Beach Wine and Food Festival\n",
      "- Mercedes-Benz Fashion Week Miami\n",
      "- Calle Ocho Festival\n",
      "- Adrienne Arsht Center for the Performing Arts\n",
      "- Olympia Theater\n",
      "- Wertheim Performing Arts Center\n",
      "- Fair Expo Center\n",
      "- Tower Theater\n",
      "- Bayfront Park Amphitheater\n",
      "- Miami International Film Festival\n",
      "Step chat produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are some places to visit in Miami:\n",
       "- Beaches and parks\n",
       "- Zoo Miami\n",
       "- Jungle Island\n",
       "- Miami Seaquarium\n",
       "- Botanic Garden\n",
       "- Key Biscayne\n",
       "- South Beach\n",
       "- Lincoln Road\n",
       "- Bayside Marketplace\n",
       "- Downtown Miami\n",
       "- Brickell City Centre\n",
       "- Art Deco District in Miami Beach\n",
       "- Miami Open\n",
       "- Art Basel\n",
       "- Winter Music Conference\n",
       "- South Beach Wine and Food Festival\n",
       "- Mercedes-Benz Fashion Week Miami\n",
       "- Calle Ocho Festival\n",
       "- Adrienne Arsht Center for the Performing Arts\n",
       "- Olympia Theater\n",
       "- Wertheim Performing Arts Center\n",
       "- Fair Expo Center\n",
       "- Tower Theater\n",
       "- Bayfront Park Amphitheater\n",
       "- Miami International Film Festival"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await wf.run(message=\"List all of the places to visit in Miami.\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat\n",
      "Step prepare_chat produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced no event\n",
      "Running step call_tool\n",
      "Calling function llama_cloud_tool with msg {'input': 'How do people in Chicago get around?'}\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: People in Chicago get around using various transportation options such as public transit systems like the Chicago Transit Authority (CTA) buses and trains, Metra commuter rail, and Pace buses. Additionally, Chicago has an extensive network of expressways and highways for those who prefer to drive. The city also offers bike-sharing systems like Divvy and has implemented an E-Scooter pilot program.\n",
      "Step chat produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "People in Chicago get around using various transportation options such as public transit systems like the Chicago Transit Authority (CTA) buses and trains, Metra commuter rail, and Pace buses. Additionally, Chicago has an extensive network of expressways and highways for those who prefer to drive. The city also offers bike-sharing systems like Divvy and has implemented an E-Scooter pilot program."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await wf.run(message=\"How do people in Chicago get around?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat\n",
      "Step prepare_chat produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced no event\n",
      "Running step call_tool\n",
      "Calling function llama_cloud_tool with msg {'input': 'What is the historical name of Los Angeles?'}\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event InputEvent\n",
      "Running step chat\n",
      "Chat message: The historical name of Los Angeles is El Pueblo de Nuestra Señora la Reina de los Ángeles.\n",
      "Step chat produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The historical name of Los Angeles is El Pueblo de Nuestra Señora la Reina de los Ángeles."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await wf.run(message=\"What is the historical name of Los Angeles?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
